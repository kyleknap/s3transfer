#!/usr/bin/env python
import os
import threading
import Queue

import boto3
from s3transfer.manager import TransferManager
from s3transfer.subscribers import BaseSubscriber

BUCKET = 'kyleknap-dump'
PREFIX = 'many-files-test/'
LOCAL_DIRECTORY = os.path.expanduser('~/temp-many-files')

SHUTDOWN = object()
NUM_THREADS = 10


class Worker(threading.Thread):
    def __init__(self, client, queue):
        self._queue = queue
        self._client = client
        threading.Thread.__init__(self)

    def run(self):
        while True:
            try:
                task  = self._queue.get(True)
                if task is SHUTDOWN:
                    break
                key, filename = task
                stream = self._client.get_object(
                    Bucket=BUCKET, Key=key)['Body']
                chunks = iter(
                    lambda: stream.read(256 * 1024 * 1024), b'')
                with open(filename, 'wb') as f:
                    for chunk in chunks:
                        f.write(chunk)
            except Queue.Empty:
                pass

class ProvideSizeSubscriber(BaseSubscriber):
    """
    A subscriber which provides the transfer size before it's queued.
    """
    def __init__(self, size):
        self.size = size

    def on_queued(self, future, **kwargs):
        future.meta.provide_transfer_size(self.size)


def baseline():
    threads = []
    queue = Queue.Queue()
    c = boto3.client('s3')

    for _ in range(NUM_THREADS):
        thread = Worker(c, queue)
        thread.setDaemon(True)
        threads.append(thread)
        thread.start()

    paginator = c.get_paginator('list_objects')
    for response in paginator.paginate(Bucket=BUCKET, Prefix=PREFIX):
        for item in response['Contents']:
            key = item['Key']
            filename = os.path.join(LOCAL_DIRECTORY, key[len(PREFIX):])
            queue.put((key, filename))

    for _ in threads:
        queue.put(SHUTDOWN)

    for thread in threads:
        thread.join()

def s3transfer_baseline():
    c = boto3.client('s3')

    paginator = c.get_paginator('list_objects')
    with TransferManager(c) as manager:
        for response in paginator.paginate(Bucket=BUCKET, Prefix=PREFIX):
            for item in response['Contents']:
                key = item['Key']
                filename = os.path.join(LOCAL_DIRECTORY, key[len(PREFIX):])
                manager.download(
                    BUCKET, key, filename,
                    subscribers=[ProvideSizeSubscriber(item['Size'])])


def main():
    #baseline()
    s3transfer_baseline()



if __name__ == '__main__':
    main()
